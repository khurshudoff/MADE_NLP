{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_oOKBQ4YqQR"
   },
   "source": [
    "## Homework №3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4AlD0wHsYqQT"
   },
   "source": [
    "### Neural Machine Translation in the wild\n",
    "In the third homework you are supposed to get the best translation you can for the EN-RU translation task.\n",
    "\n",
    "Basic approach using RNNs as encoder and decoder is implemented for you. \n",
    "\n",
    "Your ultimate task is to use the techniques we've covered, e.g.\n",
    "\n",
    "* Optimization enhancements (e.g. learning rate decay)\n",
    "\n",
    "* CNN encoder (with or without positional encoding)\n",
    "\n",
    "* attention/self-attention mechanism\n",
    "\n",
    "* pretraining the language model\n",
    "\n",
    "* [Byte Pair Encoding](https://github.com/rsennrich/subword-nmt)\n",
    "\n",
    "* or just fine-tunning BERT ;)\n",
    "\n",
    "to improve the translation quality. \n",
    "\n",
    "__Please use at least three different approaches/models and compare them (translation quality/complexity/training and evaluation time).__\n",
    "\n",
    "Write down some summary on your experiments and illustrate it with convergence plots/metrics and your thoughts. Just like you would approach a real problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8_Z_zfEHYqQU"
   },
   "outputs": [],
   "source": [
    "# You might need to install the libraries below. Do it in the desired environment\n",
    "# if you are working locally.\n",
    "\n",
    "# ! pip  install subword-nmt\n",
    "# ! pip install nltk\n",
    "# ! pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "bTgqTjcnYqQb",
    "outputId": "7c514324-e37d-4fc2-efe2-976e99738e61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset not found locally. Downloading from github.\n",
      "Файл «data.txt» уже существует — не загружается.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Thanks to YSDA NLP course team for the data\n",
    "# (who thanks tilda and deephack teams for the data in their turn)\n",
    "\n",
    "import os\n",
    "path_do_data = '../../datasets/Machine_translation_EN_RU/data.txt'\n",
    "if not os.path.exists(path_do_data):\n",
    "    print(\"Dataset not found locally. Downloading from github.\")\n",
    "    !wget https://raw.githubusercontent.com/neychev/made_nlp_course/master/datasets/Machine_translation_EN_RU/data.txt -nc\n",
    "    path_do_data = './data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IzVU5awpYqQf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'figure.figsize': (16, 12), 'font.size': 14})\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NmNfZLXfYqQj"
   },
   "source": [
    "### Main part\n",
    "__Here comes the preprocessing. Do not hesitate to use BPE or more complex preprocessing ;)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vj7GBbRHYqQj"
   },
   "outputs": [],
   "source": [
    "tokenizer_W = WordPunctTokenizer()\n",
    "def tokenize(x, tokenizer=tokenizer_W):\n",
    "    return tokenizer.tokenize(x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7JukXf04YqQm"
   },
   "outputs": [],
   "source": [
    "SRC = Field(tokenize=tokenize,\n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize=tokenize,\n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "dataset = torchtext.data.TabularDataset(\n",
    "    path=path_do_data,\n",
    "    format='tsv',\n",
    "    fields=[('trg', TRG), ('src', SRC)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yrIL1NcAYqQp"
   },
   "outputs": [],
   "source": [
    "import random as rd\n",
    "rd.seed(42)\n",
    "\n",
    "train_data, valid_data, test_data = dataset.split(split_ratio=[0.8, 0.15, 0.05],\n",
    "                                                  random_state=rd.getstate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "DpcrIKufYqQt",
    "outputId": "214a9e7c-3652-47b9-8fc2-c968b04b3b49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 40000\n",
      "Number of validation examples: 2500\n",
      "Number of testing examples: 7500\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4L-iLw8xYqQx"
   },
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 3)\n",
    "TRG.build_vocab(train_data, min_freq = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "9lbxzYUuYqQ3",
    "outputId": "588ccce6-c2da-4ef5-8d0e-0f693b3f26ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (ru) vocabulary: 9250\n",
      "Unique tokens in target (en) vocabulary: 6736\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source (ru) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qsg9kKo3YqQ6"
   },
   "source": [
    "Here are tokens from original (RU) corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "c__Df1MFYqQ7",
    "outputId": "03a6de63-924b-46f3-ba69-89fdedf1e977"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " 'радиусе',\n",
       " 'транспорта',\n",
       " 'купаться',\n",
       " 'cottages',\n",
       " '69',\n",
       " 'mirage',\n",
       " 'форум',\n",
       " 'дружбы',\n",
       " 'типичной']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC.vocab.itos[::1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QL_yGTXXYqQ9"
   },
   "source": [
    "And from target (EN) corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pKaK2SsmYqQ-",
    "outputId": "4afbc79a-3233-4bd9-f103-aa045d0ea709"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', 'palm', 'freedom', 'chi', 'nairobi', 'schwedenplatz', 'malindi']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRG.vocab.itos[::1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TgF4xdipYqRB"
   },
   "source": [
    "And here is example from train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "YxN1vnSCYqRB",
    "outputId": "52f72d15-dd4d-4753-816d-618d6c59b2e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trg': ['free', 'private', 'parking', 'is', 'available', 'on', 'site', '.'], 'src': ['на', 'прилегающей', 'территории', 'обустроена', 'бесплатная', 'частная', 'парковка', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPvd_klnYqRE"
   },
   "source": [
    "Let's check the length distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "id": "xGOeKT-0YqRE",
    "outputId": "302fcb45-c639-40a8-ede7-640e5ff0794a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length distribution in Train data\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "src_length = list(map(len, [vars(x)['src'] for x in train_data.examples]))\n",
    "trg_length = list(map(len, [vars(x)['trg'] for x in train_data.examples]))\n",
    "\n",
    "MAX_LENGTH = max(src_length + trg_length)\n",
    "\n",
    "print('Length distribution in Train data')\n",
    "plt.figure(figsize=[8, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"source length\")\n",
    "plt.hist(list(src_length), bins=20);\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"translation length\")\n",
    "plt.hist(list(trg_length), bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "id": "Xi9WcfE0YqRH",
    "outputId": "65389333-af4d-4c8a-a4b6-ee228fbdac8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length distribution in Test data\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "src_length = map(len, [vars(x)['src'] for x in test_data.examples])\n",
    "trg_length = map(len, [vars(x)['trg'] for x in test_data.examples])\n",
    "\n",
    "print('Length distribution in Test data')\n",
    "plt.figure(figsize=[8, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"source length\")\n",
    "plt.hist(list(src_length), bins=20);\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"translation length\")\n",
    "plt.hist(list(trg_length), bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z1zdl4cmYqRJ"
   },
   "source": [
    "### Model side\n",
    "__Here comes simple pipeline of NMT model learning. It almost copies the week03 practice__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_lFhz7bBYqRJ"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jlNkcG5ZYqRL",
    "outputId": "e1ab7cb0-0b50-4284-a1ff-1ee90bee8bd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rkDMbwDwYqRM"
   },
   "outputs": [],
   "source": [
    "def _len_sort_key(x):\n",
    "    return len(x.src)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device,\n",
    "    sort_key=_len_sort_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "oWaQg_rPYqRO",
    "outputId": "0269ba88-683f-4edd-87db-44c97d5f46b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 128]\n",
      "\t[.trg]:[torch.LongTensor of size 44x128]\n",
      "\t[.src]:[torch.LongTensor of size 39x128]\n",
      "torch.Size([39, 128]) torch.Size([44, 128])\n"
     ]
    }
   ],
   "source": [
    "for x in train_iterator:\n",
    "    break\n",
    "print(x)\n",
    "print(x.src.shape, x.trg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout, max_length=120):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=output_dim,\n",
    "            embedding_dim=emb_dim\n",
    "        )\n",
    "        \n",
    "        self.attn = nn.Linear(\n",
    "            in_features=emb_dim + hid_dim,\n",
    "            out_features=max_length\n",
    "        )\n",
    "        self.attn_combine = nn.Linear(\n",
    "            in_features=emb_dim + hid_dim,\n",
    "            out_features=hid_dim\n",
    "        )\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hid_dim,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(\n",
    "            in_features=hid_dim,\n",
    "            out_features=output_dim\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input)).view(1, 1, -1)\n",
    "        \n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        prediction = self.out(output.squeeze(0))\n",
    "\n",
    "        return prediction, hidden\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "        input = trg[0, :]\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            input = (trg[t] if teacher_force else top1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vzKBRZmBYqRR"
   },
   "outputs": [],
   "source": [
    "import my_network_gru_attention\n",
    "# Encoder = my_network_gru_attention.Encoder\n",
    "# Decoder = my_network_gru_attention.Decoder\n",
    "# Seq2Seq = my_network_gru_attention.Seq2Seq\n",
    "\n",
    "Encoder = my_network_gru_attention.Encoder\n",
    "AttnDecoder = AttnDecoder\n",
    "Seq2Seq = Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8RnzhPqhYqRT"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "MAX_LENGTH = MAX_LENGTH\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = AttnDecoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "# dont forget to put the model to the right device\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "3aiD6IP9YqRV",
    "outputId": "9c2b5dd1-2b2b-4cee-d6d5-d42e9af18aca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(9250, 256)\n",
       "    (gru): GRU(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): AttnDecoder(\n",
       "    (embedding): Embedding(6736, 256)\n",
       "    (attn): Linear(in_features=768, out_features=120, bias=True)\n",
       "    (attn_combine): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (gru): GRU(256, 512, num_layers=2, dropout=0.5)\n",
       "    (out): Linear(in_features=512, out_features=6736, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    # <YOUR CODE HERE>\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 1. Got 1 and 128 in dimension 0 at ../aten/src/TH/generic/THTensor.cpp:612",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-13c2137c605c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/pyvenv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-96-f645426aa6fe>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mteacher_force\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyvenv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-96-f645426aa6fe>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         attn_weights = F.softmax(\n\u001b[0;32m---> 45\u001b[0;31m             self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n\u001b[0m\u001b[1;32m     46\u001b[0m         attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n\u001b[1;32m     47\u001b[0m                                  encoder_outputs.unsqueeze(0))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 1. Got 1 and 128 in dimension 0 at ../aten/src/TH/generic/THTensor.cpp:612"
     ]
    }
   ],
   "source": [
    "model(src, trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ry-B7gt8YqRW",
    "outputId": "1d0d93d2-80cb-4655-cd99-9dc72959db95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 13,551,304 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YY07gV--YqRY"
   },
   "outputs": [],
   "source": [
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iscPHhmeYqRZ"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip, train_history=None, valid_history=None):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    history = []\n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        if i > 2:\n",
    "            break\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg sent len, batch size]\n",
    "        #output = [trg sent len, batch size, output dim]\n",
    "        \n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg sent len - 1) * batch size]\n",
    "        #output = [(trg sent len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Let's clip the gradient\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        history.append(loss.cpu().data.numpy())\n",
    "#         if (i+1)%10==0:\n",
    "        if True:\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 8))\n",
    "\n",
    "            clear_output(True)\n",
    "            ax[0].plot(history, label='train loss')\n",
    "            ax[0].set_xlabel('Batch')\n",
    "            ax[0].set_title('Train loss')\n",
    "            if train_history is not None:\n",
    "                ax[1].plot(train_history, label='general train history')\n",
    "                ax[1].set_xlabel('Epoch')\n",
    "            if valid_history is not None:\n",
    "                ax[1].plot(valid_history, label='general valid history')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.show()\n",
    "\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "haWMNkm8YqRb"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    history = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            if i > 2:\n",
    "                break\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg sent len, batch size]\n",
    "            #output = [trg sent len, batch size, output dim]\n",
    "\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg sent len - 1) * batch size]\n",
    "            #output = [(trg sent len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XaebT7bQYqRc"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FuCQxFJvYqRd"
   },
   "outputs": [],
   "source": [
    "train_history = []\n",
    "valid_history = []\n",
    "\n",
    "N_EPOCHS = 2\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "colab_type": "code",
    "id": "3vxyKipSYqRe",
    "outputId": "6a4eb378-9e0e-4380-a247-7d9697cdcdc4"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3-dimensional tensor, but got 4-dimensional tensor for argument #2 'batch2' (while checking arguments for bmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-888f7ba0850f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-c9e58d348299>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip, train_history, valid_history)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#trg = [trg sent len, batch size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyvenv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-4f7158ea2e63>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mteacher_force\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyvenv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-4f7158ea2e63>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     44\u001b[0m         attn_weights = F.softmax(\n\u001b[1;32m     45\u001b[0m             self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n\u001b[0;32m---> 46\u001b[0;31m         attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n\u001b[0m\u001b[1;32m     47\u001b[0m                                  encoder_outputs.unsqueeze(0))\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3-dimensional tensor, but got 4-dimensional tensor for argument #2 'batch2' (while checking arguments for bmm)"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP, train_history, valid_history)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    train_history.append(train_loss)\n",
    "    valid_history.append(valid_loss)\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t_qx4V_kYqRf"
   },
   "source": [
    "__Let's take a look at our network quality__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VKV4mB__YqRh"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import imp\n",
    "imp.reload(utils)\n",
    "generate_translation = utils.generate_translation\n",
    "remove_tech_tokens = utils.remove_tech_tokens\n",
    "get_text = utils.get_text\n",
    "flatten = utils.flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5uj8IIPBYqRi"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(test_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "jHCu6vBVYqRj",
    "outputId": "d1834c4b-aa87-40d0-dcf3-2e0ca1766c5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: there is a 24 - hour front desk at the property .\n",
      "Generated: the is is is and . . . . . . . . .\n",
      "\n",
      "Original: you will find a 24 - hour front desk at the property .\n",
      "Generated: the is is is and . . . . . . . . .\n",
      "\n",
      "Original: guests benefit from terrace .\n",
      "Generated: the is is is and . . . . . . . . .\n",
      "\n",
      "Original: the property offers free parking .\n",
      "Generated: the is is is and . . . . . . . . .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in [1,26, 56, 87]:\n",
    "    src = batch.src[:, idx:idx+1]\n",
    "    trg = batch.trg[:, idx:idx+1]\n",
    "    generate_translation(src, trg, model, TRG.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E0hnZA5rYqRm"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "#     \"\"\" Estimates corpora-level BLEU score of model's translations given inp and reference out \"\"\"\n",
    "#     translations, _ = model.translate_lines(inp_lines, **flags)\n",
    "#     # Note: if you experience out-of-memory error, split input lines into batches and translate separately\n",
    "#     return corpus_bleu([[ref] for ref in out_lines], translations) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UeKos6HQYqRn"
   },
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zBGZDcMYYqRo",
    "outputId": "5cb15f2e-293f-44ac-9188-7e21a9f53b8c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00,  3.02it/s]\n"
     ]
    }
   ],
   "source": [
    "original_text = []\n",
    "generated_text = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    for i, batch in tqdm.tqdm(enumerate(test_iterator)):\n",
    "        \n",
    "        if i > 2:\n",
    "            break\n",
    "\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "\n",
    "        output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "        #trg = [trg sent len, batch size]\n",
    "        #output = [trg sent len, batch size, output dim]\n",
    "\n",
    "        output = output.argmax(dim=-1)\n",
    "        \n",
    "        original_text.extend([get_text(x, TRG.vocab) for x in trg.cpu().numpy().T])\n",
    "        generated_text.extend([get_text(x, TRG.vocab) for x in output[1:].detach().cpu().numpy().T])\n",
    "\n",
    "# original_text = flatten(original_text)\n",
    "# generated_text = flatten(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "i67iuKl-YqRp",
    "outputId": "0409040e-55f6-45de-a28e-3f103b398c6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0630174991889025e-229"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_bleu([[text] for text in original_text], generated_text) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XiuFKmLlYqRr"
   },
   "source": [
    "Baseline solution BLEU score is quite low. Try to achieve at least __18__ BLEU on the test set. \n",
    "The checkpoints are:\n",
    "\n",
    "* __18__ - minimal score to submit the homework, 30% of points\n",
    "\n",
    "* __20__ - good score, 70% of points\n",
    "\n",
    "* __25__ - excellent score, 100% of points"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "homework03_Neural_Machine_Translation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "pyvenv",
   "language": "python",
   "name": "pyvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
